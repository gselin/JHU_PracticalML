---
title: "JHU-PracticalMLProject"
author: "Selin Gokalp"
date: "9/16/2020"
output:   
  html_document:
    keep_md: yes
  md_document:
    variant: markdown_github
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, fig.width=8, fig.height=6)

options(width=120)

#install.packages("rattle")

library(rattle)
library(caret)
library(rpart)
library(rpart.plot)
library(corrplot)
library(randomForest)
library(RColorBrewer)
```

## Outline of The Project

Used with a dataset provided by Human Activity Recognition (HAR) (http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har), a predictive model will be trained to anticipate what exercises were performed by humans using with the dataset, containing 159 different features.


I will follow the outline in this project:

First, I will do data processing on the dataset to be used, then explore it. I will analyze models and select the model after a brief examination to see if the selected model fits my standards. I will conclude this project where I answer the questions, and also predict classification model on the test set which is divided formerly by HAR. 

# Data Processing

## Reading the Data
After downloading the data from the data source, we can read the two csv files into two data frames.  

```{r warning=FALSE, error=FALSE}
TrainDataFirst <- read.csv("pml-training.csv")
TestDataFirst <- read.csv("pml-testing.csv")


dim(TrainDataFirst)
dim(TestDataFirst)
rm(trainFile)
rm(testFile)
```  
The training data set: `r dim(TrainDataFirst)[1]` observations and `r dim(TrainDataFirst)[2]` variables, while the testing data set: `r dim(TestDataFirst)[1]` observations and `r dim(TestDataFirst)[2]` variables. The `classe` variable in the training set is the outcome to predict.  


## Cleaning the Data
Since there is lots of NAs and meaningless data points in the dataset, I will remove the NAs from the dataset, as well as some meaningless variables.  

First, I clean the Near Zero Variance variables.
```{r warning=FALSE, error=FALSE}
NZV <- nearZeroVar(TrainDataFirst, saveMetrics = TRUE)
head(NZV, 20)

training_1 <- TrainDataFirst[, !NZV$nzv]
testing_1 <- TestDataFirst[, !NZV$nzv]


dim(training_1)
dim(testing_1)
rm(TrainDataFirst)
rm(TestDataFirst)
rm(NZV)
```  

2. I remove some features of the dataset since those do not contribute much to the accelerometer measurements.
```{r warning=FALSE, error=FALSE}
regex <- grepl("^X|timestamp|user_name", names(training_1)) #I won't do anything with user_name

training <- training_1[, !regex]
testing <- testing_1[, !regex]


rm(regex)
rm(training_1)
rm(testing_1)
dim(training)
dim(testing)
```  

3. I drop the columns that containing NA values.
```{r warning=FALSE, error=FALSE}
cond <- (colSums(is.na(training)) == 0) #set my conditions to find NAs

training <- training[, cond] #apply my conditional arguments and drop the NAs
testing <- testing[, cond] #apply my conditional arguments and drop the NAs

rm(cond)
```  


The correlation matrix of columns in the training data can be seen as: 
```{r warning=FALSE, error=FALSE}
corrplot(cor(training[, -length(names(training))]), method = "color", tl.cex = 0.5)
```  

## Partitioning Training Set  
I split the cleaned training data with the proportin of 70% to a pure training data set, thus a validation data set now contains 30% of the whole cleaned data set. To apply cross validation to the data, I will use my validation data set.
```{r warning=FALSE, error=FALSE}
set.seed(22070)


inTrain <- createDataPartition(training$classe, p = 0.70, list = FALSE) #split 70% - 30%
validation <- training[-inTrain, ]
training <- training[inTrain, ]

rm(inTrain)
```  
The Dataset now consists of `r dim(training)[2]` variables with the observations divided as following:
1. Training Data: `r dim(training)[1]` observations.  
2. Validation Data: `r dim(validation)[1]` observations.  
3. Testing Data: `r dim(testing)[1]` observations.  

# Modelling the Data

## Decision Tree Model

For activity recognition, I fit a predictive model, such as Decision Tree in this step.

```{r warning=FALSE, error=FALSE}
DecTreeModel <- rpart(classe ~ ., data = training, method = "class")

prp(DecTreeModel)
```  

I would better to see the performance results of the model on the validation dataset that I split above. To do that, I want to see the performance metrics: Sensitivity, Specificity, Positive and Negative PRedicted Values, Prevalence and Detection Rate and Prevalence.

```{r warning=FALSE, error=FALSE}
predictTree <- predict(DecTreeModel, validation, type = "class")

confusionMatrix(validation$classe, predictTree)

accuracy <- postResample(predictTree, validation$classe)
ose <- 1 - as.numeric(confusionMatrix(validation$classe, predictTree)$overall[1])


rm(predictTree)
rm(DecTreeModel)
```  

The Estimated Accuracy of the Random Forest Model is `r accuracy[1]*100`% and the Estimated Out-of-Sample Error is `r ose*100`%.  

## Random Forest Model

This time, I fit Random Forest prediction algorithm to see the activity recognition predictions. To perform the model, I choose the number for k-fold cross validation technique as 6.

```{r warning=FALSE, error=FALSE}

RandForModel <- train(classe ~ ., data = training, method = "rf", trControl = trainControl(method = "cv", 4), ntree = 250)

RandForModel
```  

The performance results of this algorithm can be found as:  
```{r warning=FALSE, error=FALSE}
predictRF <- predict(RandForModel, validation)

confusionMatrix(validation$classe, predictRF)

accuracy <- postResample(predictRF, validation$classe)
ose <- 1 - as.numeric(confusionMatrix(validation$classe, predictRF)$overall[1])


rm(predictRF)
```  

The Estimated Accuracy of the Random Forest Model is `r accuracy[1]*100`% and the Estimated Out-of-Sample Error is `r ose*100`%.  
We expected that the Random Forest would perform better, and so it does in practice.

# Predicting the Human Exercises from the Test Data Set  

I perform Random Forest model to the testing data set downloaded from the data source, not the splitting dataset but the originally downloaded from the HAR.
```{r warning=FALSE, error=FALSE}

rm(accuracy)
rm(ose)


predict(RandForModel, testing[, -length(names(testing))])
```  

## This Part is for Submitting the Project  

```{r warning=FALSE, error=FALSE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("./Desktop",i,".txt")
    write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, col.names = FALSE)
  }
}
```  


```{r warning=FALSE, error=FALSE}
pml_write_files(predict(RandForModel, testing[, -length(names(testing))]))

rm(RandForModel)
rm(training)
rm(testing)
rm(validation)
rm(pml_write_files)
```  
